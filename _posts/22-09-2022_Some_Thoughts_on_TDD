# Some Thoughts on Test Driven Development

Tests are quite useful. They can verify that our code behaves correctly. 
The be used as save guards to ensure we don't break behaviour in unexpected places.
They can also be used as documentation to explain the behaviour of a code via examples.

But Test Driven Development goes a bit further than that. In TDD - or at least the form of TDD, that i encountered - you try to test as much as possible.
From my experience the original intend of TDD was been completely lost:
If someone says they develop test driven, then they don't mean that they start with test and that they see their tests as some form of specification.
Rather, I noticest that what they actually mean is that they try to have a high test coverage - usually at about 80%.
They try to create as many tests as possible - even if some tests overlap - as long as it increases the test coverage.
And once they notice that a test fails, they rewrite it so that it works again. This in particular really cracks me up.
Either my test are the specification, in that case i should write them once and then hopefully never ever touch them again,
or they document my current behaviour but then i should only write them after im done - together with other forms of documentation.

I once had a PR rejected because I had not touched a test.
The reviewer assumed that i was impossible for me to change something without breaking at least one test.
But the change itself was a fix for a bug that only occured on a specific date at a specific time - of course non of the existing test had covered that.
We had an argument about it. He wanted me to add a test where i unit test that specific timestamp.
I was against it as it was not a edge case in the specification - just some error that occured because our original algorithm was wrong. 

This incident really got me thinking. In a sense we both where correct, but we clearly had different views as to what our tests represented.
I would want to believe that there is a place in between, where tests represent the specification but also cover a lot of the code. It might not be 80% though.
I'm still uncertain if a high code coverage is actually useful. Mostly because whenever i've seen a code coverage of 80%, it invoved a lot of (sloppy) mocks.
Creating Mocked tests is always a double edged sword, because you need to be certain that your mocked data is acutally correct.
Invalid mocks can validate everything.
I have, however, also managed a full test coverage without mocks - ok, i must clearify, i used a pure fuctional programming language (Elm).
So there would not really have been anything that needed mocking. But still.
I do acutally believe that functional programming requires less tests then OOP.
My guess for a reason, why this might be the case, would be the more expressive type system.
A type annotation is in a sense a proof (see Curryâ€“Howard correspondence). So i somehow see a relation to the whole validate vs verify topic.
But that a topic for another day.

As a small after thought: Another reason against a lot of tests is that it slows everything down - my time spent on a ticket, the time the pipeline need to complete, the time to refactor.
Where as less tests that are really meaningfull and bring a lot of back for the buck will have the same effect but will waste less of my time.
